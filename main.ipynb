{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled4.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNqeDfMyttc4vTeL4mkuvBm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shraeyas/Fake-Reviews-Detection/blob/master/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V87qcVd0hEC5"
      },
      "source": [
        "###**Clone Repository from https://github.com/Shraeyas/Fake-Reviews-Detection**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HiqR3Tt_g_6Y",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/Shraeyas/Fake-Reviews-Detection\n",
        "%cd Fake-Reviews-Detection"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TOhNn5ClhM04"
      },
      "source": [
        "###**Install, Create and activate a Python Virtual Environment**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sda60a56hWGc",
        "colab": {}
      },
      "source": [
        "!pip install virtualenv\n",
        "!virtualenv venv\n",
        "!. venv/bin/activate"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QoJdFJtdiZzA"
      },
      "source": [
        "###**Install Requirements by executing the following command**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XuCSOpRciir7",
        "colab": {}
      },
      "source": [
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OxjB1VbShyWp"
      },
      "source": [
        "###**After Restarting runtime, execute the Following Commands**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ysa_0Ua4h47p",
        "colab": {}
      },
      "source": [
        "%cd Fake-Reviews-Detection\n",
        "!. venv/bin/activate\n",
        "!pip install -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8y7TBIzfrK29",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**Import necessary libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjc9NaMnoSrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import pickle\n",
        "import codecs\n",
        "from progressBar import printProgressBar\n",
        "import argparse\n",
        "import shutil\n",
        "\n",
        "#Following code is for debugging\n",
        "\n",
        "try :\n",
        "  parser = argparse.ArgumentParser()\n",
        "  parser.add_argument('--force-retrain', help=\"Set value to 1 if you wish to retrain the models.\")\n",
        "  parser.add_argument('--debug', help=\"Debug.\")\n",
        "  args = parser.parse_args()\n",
        "  force_retrain = args.force_retrain\n",
        "  debug = args.debug\n",
        "\n",
        "except :\n",
        "  force_retrain = None\n",
        "  debug = None\n",
        "\n",
        "try :\n",
        "    shutil.unpack_archive(\"models.zip\", \"models\", \"zip\")\n",
        "\n",
        "except :\n",
        "    pass\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqcazybmotLK",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### **Read Dataset**\n",
        "---\n",
        "#### **Dataset Download Link : https://www.kaggle.com/lievgarcia/amazon-reviews**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmRgGaYgontN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with codecs.open(\"amazon_dataset_1.csv\", \"r\",encoding='utf-8', errors='ignore') as file_dat:\n",
        "     dataset = pd.read_csv(file_dat)\n",
        "\n",
        "len_dataset = math.floor(len(dataset)/1)\n",
        "\n",
        "y = dataset.iloc[:,1:2].values\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x66G7idto7Qf",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### **Download nltk Libraries**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI2W6TZ9pJ67",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('maxent_treebank_pos_tagger')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "print(\"\\n---------------------------------------------------------------------------------------\\n\")\n",
        "\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QujuFORmpOLn",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### **Tokenization and Stemming**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axcx-au6pRJv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"\\nPerforming Tokenization and Stemming.\")\n",
        "load_from_disk = False\n",
        "corpus=[]\n",
        "num = 0\n",
        "\n",
        "for i in range(0, math.floor(len_dataset)) :\n",
        "    if not debug :\n",
        "        printProgressBar(iteration = num, total = len_dataset, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
        "        num = num + 1\n",
        "\n",
        "    if os.path.exists(os.path.join(\"models\", \"corpus.sav\")) and force_retrain == None :\n",
        "        load_from_disk = True\n",
        "        continue\n",
        "\n",
        "\n",
        "    review = re.sub('[^a-zA-Z]',' ',dataset['REVIEW_TEXT'][i])\n",
        "    review = review.lower()\n",
        "    review = review.split()\n",
        "    #print (review)\n",
        "    review = [word for word in review if not word in set(stopwords.words('english'))]\n",
        "    ps = PorterStemmer()\n",
        "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
        "    review = ' '.join(review)\n",
        "    corpus.append(review)\n",
        "\n",
        "filename = 'corpus.sav'\n",
        "if load_from_disk == False :\n",
        "    pickle.dump(corpus, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "if load_from_disk :\n",
        "    corpus = pickle.load(open(os.path.join(\"models\", filename), 'rb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZDO8UFzpW3C",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### **Count Vectorization**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_V_8IFh8pZan",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "cv=CountVectorizer(max_features=3000)\n",
        "X=cv.fit_transform(corpus).toarray()\n",
        "y=dataset.iloc[:len_dataset,1]\n",
        "\n",
        "filename = 'countvectorizer.sav'\n",
        "pickle.dump(cv, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMzPsnUjpbyg",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "#### **POS Tagging**\n",
        "---\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqjI06Popkau",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def POS_Tagging(sentence):\n",
        "    tagged_list = []\n",
        "    tags = []\n",
        "    count_verbs = 0\n",
        "    count_nouns = 0\n",
        "    text=nltk.word_tokenize(sentence)\n",
        "    tagged_list = (nltk.pos_tag(text))\n",
        "\n",
        "    tags = [x[1] for x in tagged_list]\n",
        "    for each_item in tags:\n",
        "        if each_item in ['VERB','VB','VBN','VBD','VBZ','VBG','VBP']:\n",
        "            count_verbs+=1\n",
        "        elif each_item in ['NOUN','NNP','NN','NUM','NNS','NP','NNPS']:\n",
        "            count_nouns+=1\n",
        "        else:\n",
        "            continue\n",
        "    if count_verbs > count_nouns:\n",
        "        sentence = 'F'\n",
        "    else:\n",
        "        sentence = 'T'\n",
        "\n",
        "    return sentence\n",
        "\n",
        "w, h = 2, len_dataset;\n",
        "pos_tag = [[0 for x in range(w)] for y in range(h)]\n",
        "num = 0\n",
        "\n",
        "load_from_disk = False\n",
        "filename = 'pos_tag.sav'\n",
        "print (\"\\n\\nPerforming POS Tagging.\")\n",
        "for i in range(0,len_dataset):\n",
        "    if not debug :\n",
        "        printProgressBar(iteration = num, total = len_dataset, prefix = 'Progress:', suffix = 'Complete', length = 50)\n",
        "        num = num + 1\n",
        "\n",
        "    if os.path.exists(os.path.join(\"models\", filename)) and force_retrain == None :\n",
        "        load_from_disk = True\n",
        "        continue\n",
        "\n",
        "    text = dataset['REVIEW_TEXT'][i]\n",
        "    sentence = POS_Tagging(text)\n",
        "\n",
        "    if sentence == 'T':\n",
        "        pos_tag[i][0] = 1\n",
        "        pos_tag[i][1] = 0\n",
        "        #X[i].insert(1)\n",
        "        #X[i].insert(0)\n",
        "    else:\n",
        "        pos_tag[i][0] = 0\n",
        "        pos_tag[i][1] = 1\n",
        "\n",
        "    #print (pos_tag[i])\n",
        "        #X[i].insert(0)\n",
        "        #X[i].insert(1)\n",
        "\n",
        "\n",
        "if load_from_disk == False :\n",
        "    pickle.dump(pos_tag, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "if load_from_disk :\n",
        "    pos_tag = pickle.load(open(os.path.join(\"models\", filename), 'rb'))\n",
        "\n",
        "X = np.append(X, pos_tag, axis = 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWxr_ZHUpqwL",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**Label Encoding**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42bq7ZOPptIl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "le = LabelEncoder()\n",
        "y = le.fit_transform(y)\n",
        "\n",
        "w, h = 3, len_dataset;\n",
        "new_col = [[0 for x in range(w)] for y in range(h)]\n",
        "num = 0\n",
        "\n",
        "test = dict()\n",
        "test_num = 0\n",
        "\n",
        "for i in range(0, len_dataset):\n",
        "    new_col[i][0] = dataset[\"RATING\"][i]\n",
        "    new_col[i][1] = dataset[\"VERIFIED_PURCHASE\"][i]\n",
        "    new_col[i][2] = dataset[\"PRODUCT_CATEGORY\"][i]\n",
        "\n",
        "    if new_col[i][2] not in test.keys() :\n",
        "        test[new_col[i][2]] = 1\n",
        "        test_num = test_num + 1\n",
        "\n",
        "        #print (new_col[i][2])\n",
        "\n",
        "#print (test_num)\n",
        "\n",
        "new_col = np.array(new_col)\n",
        "\n",
        "labelEncoder = LabelEncoder()\n",
        "new_col[:, 0] = labelEncoder.fit_transform(new_col[:, 0])\n",
        "filename = 'labelencoder_1.sav'\n",
        "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "new_col[:, 1] = labelEncoder.fit_transform(new_col[:, 1])\n",
        "filename = 'labelencoder_2.sav'\n",
        "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "new_col[:, 2] = labelEncoder.fit_transform(new_col[:, 2])\n",
        "filename = 'labelencoder_3.sav'\n",
        "pickle.dump(labelEncoder, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56DhZmyzpy_N",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**OneHotEncoder / Column Transformer**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZISni_P0p1e0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ct1 = ColumnTransformer([(\"Rating\", OneHotEncoder(), [0])], remainder = 'passthrough')\n",
        "new_col = ct1.fit_transform(new_col)\n",
        "new_col = new_col.astype(np.float32)\n",
        "filename = 'columntransformer1.sav'\n",
        "pickle.dump(ct1, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "ct2 = ColumnTransformer([(\"Verified Purchase\", OneHotEncoder(), [5])], remainder = 'passthrough')\n",
        "new_col = ct2.fit_transform(new_col)\n",
        "new_col = new_col.astype(np.float32)\n",
        "filename = 'columntransformer2.sav'\n",
        "pickle.dump(ct2, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "ct3 = ColumnTransformer([(\"Category\", OneHotEncoder(), [7])], remainder = 'passthrough')\n",
        "new_col = ct3.fit_transform(new_col)\n",
        "new_col = new_col.toarray()\n",
        "new_col = new_col.astype(np.float32)\n",
        "filename = 'columntransformer3.sav'\n",
        "pickle.dump(ct3, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "new_col = new_col.astype(np.uint8)\n",
        "X = X.astype(np.uint8)\n",
        "X = np.append(X, new_col, axis=1).astype(np.uint8)\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tqsx4rJ_qGgt",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**Split in Train and Test Set**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oI4JnsNmqJEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)\n",
        "\n",
        "print (\"Done.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOsYZODvqLXb",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**Training Classifiers**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hI53PdO-qSQF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"\\n\\nTraining Classifier on Bernoulli Naive Bayes.\")\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "\n",
        "bernoullinb = None\n",
        "if os.path.exists(os.path.join(\"models\", \"bernoullinb.sav\")) and force_retrain == None:\n",
        "    bernoullinb = pickle.load(open(os.path.join(\"models\", \"bernoullinb.sav\"), \"rb\"))\n",
        "\n",
        "else :\n",
        "    bernoullinb = BernoulliNB(alpha = 1.0, binarize = 0.0, fit_prior = True, class_prior = None)\n",
        "    bernoullinb.fit(X_train,y_train)\n",
        "\n",
        "    filename = 'bernoullinb.sav'\n",
        "    pickle.dump(bernoullinb, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "y_pred_bernoulli = bernoullinb.predict(X_test)\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print (\"\\nAccuracy of Bernoulli Naive Bayes is : \")\n",
        "print (accuracy_score(y_test, y_pred_bernoulli) * 100)\n",
        "\n",
        "print (\"\\n\\nTraining Classifier on Support Vector Machine.\")\n",
        "from sklearn.svm import SVC # \"Support Vector Classifier\"\n",
        "\n",
        "clf = None\n",
        "\n",
        "if os.path.exists(os.path.join(\"models\", \"SVM.sav\")) and force_retrain == None:\n",
        "    clf = pickle.load(open(os.path.join(\"models\", \"SVM.sav\"), \"rb\"))\n",
        "    y_pred_svc = pickle.load(open(os.path.join(\"models\", \"SVM_y_pred.sav\"), \"rb\"))\n",
        "\n",
        "else :\n",
        "    clf = SVC(kernel='rbf')\n",
        "    clf.fit(X_train, y_train)\n",
        "\n",
        "    filename = 'SVM.sav'\n",
        "    pickle.dump(clf, open(os.path.join(\"models\", filename), 'wb'))\n",
        "\n",
        "    y_pred_svc = clf.predict(X_test)\n",
        "    pickle.dump(y_pred_svc, open(os.path.join(\"models\", \"SVM_y_pred.sav\"), 'wb'))\n",
        "\n",
        "print(\"Done.\")\n",
        "\n",
        "shutil.make_archive(\"models\", 'zip', \"models\")\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "print (\"\\nAccuracy of Support Vector Machine is : \")\n",
        "print(accuracy_score(y_test, y_pred_svc) * 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncR457cPqXN7",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "####**Plot Graphs**\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyIMWHlPqW8b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from graph import plot2d, plot_comp\n",
        "\n",
        "X = np.concatenate((X_train, X_test))\n",
        "y = np.concatenate((y_train, y_test))\n",
        "\n",
        "plot2d(X_test, y_test, y_pred_bernoulli, y_pred_svc)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}